{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN: Fashion-MNIST \n",
    "\n",
    "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:\n",
    "\n",
    "\n",
    "| Label |\tDescription|\n",
    "|---|------------------|\n",
    "| 0 |\tT-shirt/top    |\n",
    "| 1 |\tTrouser        |\n",
    "| 2 |\tPullover       |\n",
    "| 3 |\tDress          |\n",
    "| 4 |\tCoat           |\n",
    "| 5 |\tSandal         |\n",
    "| 6 |\tShirt          |\n",
    "| 7 |\tSneaker        |\n",
    "| 8 |\tBag            |\n",
    "| 9 |\tAnkle boot     |\n",
    "\n",
    "See [keras docs](https://keras.io/datasets/).\n",
    "\n",
    "In this exercise we will train a CNN on the dataset.\n",
    "You can use either TensorFlow or Keras.\n",
    "\n",
    "We'll get the data via [`keras.datasets`](https://keras.io/datasets/).\n",
    "It takes some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the images to a float32 between 0 and 1 and reshape to 28x28x1 (only one channel for black and white) because 2D convolutions expect 3D images (3rd dimension is channel or image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 126,
       "width": 126
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, x_train.shape[0])\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(x_train[i, :, :, 0], cmap='gray_r')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "print(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** One-hot encode the y values. You can use your own function or `keras.utils.to_categorical`. Don't forget to convert both y arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Build and train the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 32s 527us/step - loss: 0.9074 - acc: 0.6764 - val_loss: 0.5861 - val_acc: 0.7815\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 30s 506us/step - loss: 0.5834 - acc: 0.7829 - val_loss: 0.5041 - val_acc: 0.8186\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 34s 563us/step - loss: 0.5129 - acc: 0.8113 - val_loss: 0.4570 - val_acc: 0.8359\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 32s 537us/step - loss: 0.4717 - acc: 0.8266 - val_loss: 0.4234 - val_acc: 0.8511\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 30s 500us/step - loss: 0.4427 - acc: 0.8405 - val_loss: 0.4047 - val_acc: 0.8552\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 29s 484us/step - loss: 0.4208 - acc: 0.8487 - val_loss: 0.3898 - val_acc: 0.8621\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 31s 516us/step - loss: 0.3996 - acc: 0.8563 - val_loss: 0.3741 - val_acc: 0.8669\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 31s 524us/step - loss: 0.3866 - acc: 0.8594 - val_loss: 0.3594 - val_acc: 0.8735\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 36s 599us/step - loss: 0.3751 - acc: 0.8650 - val_loss: 0.3559 - val_acc: 0.8749\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 37s 612us/step - loss: 0.3639 - acc: 0.8677 - val_loss: 0.3447 - val_acc: 0.8776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x130523780>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=50,\n",
    "    epochs=10,\n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 183us/step\n",
      "Test loss: 0.344747866154\n",
      "Test accuracy: 0.8776\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I trained a CNN I got this accuracy of ~87% on the test set, see if you can top it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciPy]",
   "language": "python",
   "name": "conda-env-DataSciPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
