{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN: Fashion-MNIST \n",
    "\n",
    "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:\n",
    "\n",
    "\n",
    "| Label |\tDescription|\n",
    "|---|------------------|\n",
    "| 0 |\tT-shirt/top    |\n",
    "| 1 |\tTrouser        |\n",
    "| 2 |\tPullover       |\n",
    "| 3 |\tDress          |\n",
    "| 4 |\tCoat           |\n",
    "| 5 |\tSandal         |\n",
    "| 6 |\tShirt          |\n",
    "| 7 |\tSneaker        |\n",
    "| 8 |\tBag            |\n",
    "| 9 |\tAnkle boot     |\n",
    "\n",
    "See [keras docs](https://keras.io/datasets/).\n",
    "\n",
    "In this exercise we will train a CNN on the dataset.\n",
    "You can use either TensorFlow or Keras.\n",
    "The [solution](../solutions/CNN.ipynb) is in the `solutions` folder.\n",
    "\n",
    "We'll get the data via [`keras.datasets`](https://keras.io/datasets/).\n",
    "It takes some time to download, and you should also install keras (`conda install keras`) rather than use the keras supplied with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the images to a float32 between 0 and 1 and reshape to 28x28x1 (only one channel for black and white) because 2D convolutions expect 3D images (3rd dimension is channel or image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 126,
       "width": 126
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, x_train.shape[0])\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(x_train[i, :, :, 0], cmap='gray_r')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "print(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** One-hot encode the y values. You can use your own function or `keras.utils.to_categorical`. Don't forget to convert both y arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Build and train the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I trained a CNN I got this accuracy of ~87% on the test set, see if you can top it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciPy]",
   "language": "python",
   "name": "conda-env-DataSciPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
